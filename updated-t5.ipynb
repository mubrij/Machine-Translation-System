{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bello/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-15 01:43:55.071233: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-15 01:43:55.077890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-15 01:43:55.086752: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-15 01:43:55.089453: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-15 01:43:55.096115: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-15 01:43:55.530361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to /home/bello/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "def load_model_and_tokenizer(model_name, device_map='auto'):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    model.config.use_cache = False\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.padding_side = 'right'\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load data\n",
    "def load_data(train_path, test_path):\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    return train, test\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(train, tokenizer):\n",
    "    template = \"translate from Dyula to French: {dyu}\"\n",
    "    train[\"prompt\"] = train.apply(lambda row: template.format(dyu=row['dyu']), axis=1)\n",
    "    train_ds_raw = Dataset.from_pandas(train, split=\"train\")\n",
    "    \n",
    "    tokenized_source_training = train_ds_raw.map(\n",
    "        lambda x: tokenizer(x[\"prompt\"], truncation=True), \n",
    "        batched=True, remove_columns=['fr', 'dyu', 'prompt'])\n",
    "    \n",
    "    source_lengths_training = [len(x) for x in tokenized_source_training[\"input_ids\"]]\n",
    "    target_lengths_training = [len(tokenizer(x, truncation=True)[\"input_ids\"]) for x in train[\"fr\"]]\n",
    "    \n",
    "    max_source_length = max(source_lengths_training)\n",
    "    max_target_length = max(target_lengths_training)\n",
    "    \n",
    "    return train_ds_raw, max_source_length, max_target_length\n",
    "\n",
    "# Tokenize function\n",
    "def preprocess_function(sample, tokenizer, max_source_length, max_target_length, padding=\"max_length\"):\n",
    "    model_inputs = tokenizer(sample[\"prompt\"], max_length=max_source_length, padding=padding, truncation=True)\n",
    "    labels = tokenizer(text_target=sample[\"fr\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "    \n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Metric computation\n",
    "def compute_metrics(eval_preds, tokenizer, metric):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "# Create k-fold datasets\n",
    "def create_kfold_datasets(dataset, n_splits=5, shuffle=True, random_state=42):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "    \n",
    "    fold_datasets = []\n",
    "    for train_idx, val_idx in kf.split(dataset):\n",
    "        train_fold = dataset.select(train_idx)\n",
    "        val_fold = dataset.select(val_idx)\n",
    "        fold_datasets.append((train_fold, val_fold))\n",
    "    \n",
    "    return fold_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bello/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/bello/.local/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/8065 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8065/8065 [00:00<00:00, 95609.34 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8065/8065 [00:00<00:00, 32492.75 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................Training fold 1/5.....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bello/.local/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/bello/.local/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/bello/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='4040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 216/4040 01:10 < 20:59, 3.04 it/s, Epoch 0.53/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # Set up parameters\n",
    "    model_name = 'google/mt5-base'\n",
    "    train_path = \"/home/bello/workspace/works/final_train_df.csv\"\n",
    "    test_path = \"/home/bello/workspace/works/final_val_df.csv\"\n",
    "    LOCAL_SAVE_DIR = \"dyu_to_fr_model\"\n",
    "    batch_size = 8\n",
    "    n_folds = 5\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "\n",
    "    # Load and preprocess data\n",
    "    train, test = load_data(train_path, test_path)\n",
    "    train_ds_raw, max_source_length, max_target_length = preprocess_data(train, tokenizer)\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_train_ds = train_ds_raw.map(\n",
    "        lambda x: preprocess_function(x, tokenizer, max_source_length, max_target_length),\n",
    "        batched=True, \n",
    "        remove_columns=['fr', 'dyu', 'prompt']\n",
    "    )\n",
    "\n",
    "    # Prepare for training\n",
    "    label_pad_token_id = -100\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=label_pad_token_id, pad_to_multiple_of=8)\n",
    "    metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "    # Create k-fold datasets\n",
    "    fold_datasets = create_kfold_datasets(tokenized_train_ds, n_splits=n_folds)\n",
    "\n",
    "    # Train the model for each fold\n",
    "    for fold, (train_dataset, val_dataset) in enumerate(fold_datasets, 1):\n",
    "        print(f\".....................................Training fold {fold}/{n_folds}.....................................\")\n",
    "        \n",
    "        # Initialize a new model for each fold\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        \n",
    "        # Set up the trainer for this fold\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=f\"{LOCAL_SAVE_DIR}/fold_{fold}\",\n",
    "            learning_rate=5e-5,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            generation_max_length=max_target_length,\n",
    "            weight_decay=0.01,\n",
    "            num_train_epochs=10,\n",
    "            predict_with_generate=True,\n",
    "            fp16=False,\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=500,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True\n",
    "        )\n",
    "        \n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=lambda eval_preds: compute_metrics(eval_preds, tokenizer, metric),\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the model for this fold\n",
    "        trainer.save_model(f\"{LOCAL_SAVE_DIR}/fold_{fold}\")\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(f\"Evaluation results for fold {fold}:\", eval_results)\n",
    "\n",
    "    # Select the best model based on BLEU score\n",
    "    best_fold = None\n",
    "    best_metric = float('-inf')\n",
    "\n",
    "    for fold in range(1, n_folds + 1):\n",
    "        model_path = f\"{LOCAL_SAVE_DIR}/fold_{fold}\"\n",
    "        if os.path.exists(model_path):\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            \n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                tokenizer=tokenizer,\n",
    "                compute_metrics=lambda eval_preds: compute_metrics(eval_preds, tokenizer, metric),\n",
    "            )\n",
    "            \n",
    "            eval_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "            current_metric = eval_results['eval_bleu']\n",
    "            \n",
    "            if current_metric > best_metric:\n",
    "                best_metric = current_metric\n",
    "                best_fold = fold\n",
    "\n",
    "    print(f\"Best model is from fold {best_fold} with BLEU score: {best_metric}\")\n",
    "\n",
    "    # Save the best model\n",
    "    best_model_path = f\"{LOCAL_SAVE_DIR}/fold_{best_fold}\"\n",
    "    best_model = AutoModelForSeq2SeqLM.from_pretrained(best_model_path)\n",
    "    best_tokenizer = AutoTokenizer.from_pretrained(best_model_path)\n",
    "    best_model.save_pretrained(f\"{LOCAL_SAVE_DIR}/best_model\")\n",
    "    best_tokenizer.save_pretrained(f\"{LOCAL_SAVE_DIR}/best_model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
