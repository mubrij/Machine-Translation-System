{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9073798,"sourceType":"datasetVersion","datasetId":5473494},{"sourceId":9105282,"sourceType":"datasetVersion","datasetId":5495381}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -q trl evaluate sacrebleu","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:39:41.794160Z","iopub.execute_input":"2024-08-13T11:39:41.794820Z","iopub.status.idle":"2024-08-13T11:39:54.460749Z","shell.execute_reply.started":"2024-08-13T11:39:41.794786Z","shell.execute_reply":"2024-08-13T11:39:54.459697Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration,AutoTokenizer\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback\nimport yaml\nfrom datasets import load_from_disk\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\nimport wandb\nimport numpy as np\nfrom datasets import Dataset\nfrom datasets import concatenate_datasets\nimport pandas as pd\nimport seaborn as sns\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import DataCollatorForSeq2Seq\nimport evaluate\n\ndef load_model_and_tokenizer(model_name,tokenizer_name,device_map:str='auto'):\n\n    model = T5ForConditionalGeneration.from_pretrained(\n        model_name,\n        )\n    model.config.use_cache = False\n\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,trust_remote_code=True)\n\n    tokenizer.padding_side='right'\n\n    return model, tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:39:54.463218Z","iopub.execute_input":"2024-08-13T11:39:54.463948Z","iopub.status.idle":"2024-08-13T11:40:02.383820Z","shell.execute_reply.started":"2024-08-13T11:39:54.463910Z","shell.execute_reply":"2024-08-13T11:40:02.383015Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-08-13 11:39:57.409515: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-13 11:39:57.409600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-13 11:39:57.411098: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = 't5-base'\nmodel, tokenizer = load_model_and_tokenizer(model_name,model_name)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:02.384935Z","iopub.execute_input":"2024-08-13T11:40:02.385285Z","iopub.status.idle":"2024-08-13T11:40:04.335890Z","shell.execute_reply.started":"2024-08-13T11:40:02.385253Z","shell.execute_reply":"2024-08-13T11:40:04.335063Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/melio-dataset/final_train_df.csv\")\ntest = pd.read_csv(\"/kaggle/input/melio-dataset/final_val_df.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:04.337532Z","iopub.execute_input":"2024-08-13T11:40:04.337812Z","iopub.status.idle":"2024-08-13T11:40:04.380974Z","shell.execute_reply.started":"2024-08-13T11:40:04.337789Z","shell.execute_reply":"2024-08-13T11:40:04.380100Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                        dyu                             fr\n0            A bi ji min na              Il boit de l’eau.\n1  A le dalakolontɛ lon bɛ.         Il se plaint toujours.\n2              Mun? Fɛn dɔ.          Quoi ? Quelque chose.\n3   O bɛ bi bɔra fo Gubeta.  Tous sortent excepté Gubetta.\n4   A ale lo bi da bugɔ la!      Ah ! c’est lui… il sonne…","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dyu</th>\n      <th>fr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A bi ji min na</td>\n      <td>Il boit de l’eau.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A le dalakolontɛ lon bɛ.</td>\n      <td>Il se plaint toujours.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Mun? Fɛn dɔ.</td>\n      <td>Quoi ? Quelque chose.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>O bɛ bi bɔra fo Gubeta.</td>\n      <td>Tous sortent excepté Gubetta.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A ale lo bi da bugɔ la!</td>\n      <td>Ah ! c’est lui… il sonne…</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.decode(tokenizer.encode('Å'))","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:06.809495Z","iopub.execute_input":"2024-08-13T11:40:06.810238Z","iopub.status.idle":"2024-08-13T11:40:06.817083Z","shell.execute_reply.started":"2024-08-13T11:40:06.810206Z","shell.execute_reply":"2024-08-13T11:40:06.816000Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'<unk></s>'"},"metadata":{}}]},{"cell_type":"code","source":"unknown_chars = [\n    'Å', 'ɔ', 'Ê', 'ŋ', 'α', 'Á', 'Ô', 'ā', '̀', 'ú', '̂', 'Â', 'í', 'ò',\n    '̧', 'Š', 'œ', 'Ō', 'ɲ', 'ë', 'ł', 'Ɛ', 'ñ', 'ū', 'ň', '́', 'ễ', 'Ɔ',\n    'ʻ', 'Ç', 'ō', 'ï', 'Ɲ', 'ɛ', 'Č', 'À', 'ã'\n]\n\nmapper = {char: f'<extra_id_{i}>' for i, char in enumerate(unknown_chars)}\nunmapper = {v: k for k, v in mapper.items()}","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:07.570654Z","iopub.execute_input":"2024-08-13T11:40:07.571611Z","iopub.status.idle":"2024-08-13T11:40:07.579242Z","shell.execute_reply.started":"2024-08-13T11:40:07.571571Z","shell.execute_reply":"2024-08-13T11:40:07.578113Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def unmap_unknown_chars(text, unmapper):\n    \"\"\"Unmap characters in a string based on the reverse mapper dictionary.\"\"\"\n    for extra_id, char in unmapper.items():\n        text = text.replace(extra_id, char)\n    return text\n\ndef map_unknown_chars(text, mapper):\n    \"\"\"Map unknown characters in the text to their corresponding extra_id tokens.\"\"\"\n    mapped_text = []\n    for char in text:\n        if char in mapper:\n            mapped_text.append(mapper[char])\n        else:\n            mapped_text.append(char)\n    return ''.join(mapped_text)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:08.113297Z","iopub.execute_input":"2024-08-13T11:40:08.114289Z","iopub.status.idle":"2024-08-13T11:40:08.120486Z","shell.execute_reply.started":"2024-08-13T11:40:08.114252Z","shell.execute_reply":"2024-08-13T11:40:08.119531Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train['dyu'] = train['dyu'].apply(lambda x: map_unknown_chars(x, mapper))\ntrain['fr'] = train['fr'].apply(lambda x: map_unknown_chars(x, mapper))","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:08.465233Z","iopub.execute_input":"2024-08-13T11:40:08.465975Z","iopub.status.idle":"2024-08-13T11:40:08.569297Z","shell.execute_reply.started":"2024-08-13T11:40:08.465942Z","shell.execute_reply":"2024-08-13T11:40:08.568344Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:08.792890Z","iopub.execute_input":"2024-08-13T11:40:08.793802Z","iopub.status.idle":"2024-08-13T11:40:08.802954Z","shell.execute_reply.started":"2024-08-13T11:40:08.793768Z","shell.execute_reply":"2024-08-13T11:40:08.802034Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                dyu  \\\n0                                    A bi ji min na   \n1  A le dalakolont<extra_id_33> lon b<extra_id_33>.   \n2               Mun? F<extra_id_33>n d<extra_id_1>.   \n3    O b<extra_id_33> bi b<extra_id_1>ra fo Gubeta.   \n4                A ale lo bi da bug<extra_id_1> la!   \n\n                              fr  \n0              Il boit de l’eau.  \n1         Il se plaint toujours.  \n2          Quoi ? Quelque chose.  \n3  Tous sortent excepté Gubetta.  \n4      Ah ! c’est lui… il sonne…  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dyu</th>\n      <th>fr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A bi ji min na</td>\n      <td>Il boit de l’eau.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A le dalakolont&lt;extra_id_33&gt; lon b&lt;extra_id_33&gt;.</td>\n      <td>Il se plaint toujours.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Mun? F&lt;extra_id_33&gt;n d&lt;extra_id_1&gt;.</td>\n      <td>Quoi ? Quelque chose.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>O b&lt;extra_id_33&gt; bi b&lt;extra_id_1&gt;ra fo Gubeta.</td>\n      <td>Tous sortent excepté Gubetta.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A ale lo bi da bug&lt;extra_id_1&gt; la!</td>\n      <td>Ah ! c’est lui… il sonne…</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# load the dataset, we'll use the opus_books dataset\ndataset = Dataset.from_pandas(train)\ndataset = dataset.train_test_split(test_size=0.2)\ndataset\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:09.105754Z","iopub.execute_input":"2024-08-13T11:40:09.106129Z","iopub.status.idle":"2024-08-13T11:40:09.135387Z","shell.execute_reply.started":"2024-08-13T11:40:09.106101Z","shell.execute_reply":"2024-08-13T11:40:09.134424Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['dyu', 'fr'],\n        num_rows: 6452\n    })\n    test: Dataset({\n        features: ['dyu', 'fr'],\n        num_rows: 1613\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# write the appropriate format before feeding to t5 model, also perform tokenization\n\nsource_lang = \"dyu\"\ntarget_lang = \"fr\"\nprefix = \"translate from Dyula to French: \"\n\ndef preprocess_function(examples):\n    inputs = [prefix + example for example in examples[source_lang]]\n    targets = [example for example in examples[target_lang]]\n    model_inputs = tokenizer(inputs, text_target=targets, max_length=300, truncation=True)\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:09.552643Z","iopub.execute_input":"2024-08-13T11:40:09.553499Z","iopub.status.idle":"2024-08-13T11:40:09.558908Z","shell.execute_reply.started":"2024-08-13T11:40:09.553468Z","shell.execute_reply":"2024-08-13T11:40:09.557963Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Formate and Tokenize the dataset\ntokenized_dataset = dataset.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:10.069374Z","iopub.execute_input":"2024-08-13T11:40:10.070117Z","iopub.status.idle":"2024-08-13T11:40:10.852199Z","shell.execute_reply.started":"2024-08-13T11:40:10.070085Z","shell.execute_reply":"2024-08-13T11:40:10.851251Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6452 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51c89b25081c4fb08402b20401853756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1613 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"673c258088c145118fa1906c255d7a2f"}},"metadata":{}}]},{"cell_type":"code","source":"# collate the dataset\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model = model)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:10.853646Z","iopub.execute_input":"2024-08-13T11:40:10.853932Z","iopub.status.idle":"2024-08-13T11:40:10.858148Z","shell.execute_reply.started":"2024-08-13T11:40:10.853908Z","shell.execute_reply":"2024-08-13T11:40:10.857260Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# This is our metrics for calculating bleu score\nmetric = evaluate.load(\"sacrebleu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:11.845709Z","iopub.execute_input":"2024-08-13T11:40:11.846629Z","iopub.status.idle":"2024-08-13T11:40:12.586348Z","shell.execute_reply.started":"2024-08-13T11:40:11.846595Z","shell.execute_reply":"2024-08-13T11:40:12.585609Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:12.624970Z","iopub.execute_input":"2024-08-13T11:40:12.625948Z","iopub.status.idle":"2024-08-13T11:40:12.634432Z","shell.execute_reply.started":"2024-08-13T11:40:12.625918Z","shell.execute_reply":"2024-08-13T11:40:12.633554Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback\nimport numpy as np\n\n# Calculate the number of training steps per epoch\nepoch_steps = int(np.ceil(len(tokenized_dataset['train']) / 16))\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"dyu_fr_model\",\n    learning_rate=3e-5,  # Initial learning rate\n    eval_steps=epoch_steps // 2,\n    save_steps=epoch_steps,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    weight_decay=0.01,\n    num_train_epochs=3,\n    evaluation_strategy=\"steps\",\n    logging_steps=epoch_steps // 4,\n    logging_strategy=\"steps\",\n    save_strategy=\"steps\",\n    save_total_limit=3,\n    greater_is_better=True,\n    predict_with_generate=True,\n    fp16=True,\n    load_best_model_at_end=True,\n    warmup_steps=epoch_steps // 10,\n    lr_scheduler_type='cosine',\n    gradient_accumulation_steps=2,\n    gradient_checkpointing=True,\n    label_smoothing_factor=0.1,\n    optim=\"adafactor\",\n\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]  # Stop if no improvement in 10 evaluations\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:13.465421Z","iopub.execute_input":"2024-08-13T11:40:13.466326Z","iopub.status.idle":"2024-08-13T11:40:13.887483Z","shell.execute_reply.started":"2024-08-13T11:40:13.466296Z","shell.execute_reply":"2024-08-13T11:40:13.886485Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# perform training\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:15.372389Z","iopub.execute_input":"2024-08-13T11:40:15.373311Z","iopub.status.idle":"2024-08-13T11:40:15.387195Z","shell.execute_reply.started":"2024-08-13T11:40:15.373275Z","shell.execute_reply":"2024-08-13T11:40:15.386366Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:40:19.081882Z","iopub.execute_input":"2024-08-13T11:40:19.082740Z","iopub.status.idle":"2024-08-13T11:57:40.144235Z","shell.execute_reply.started":"2024-08-13T11:40:19.082707Z","shell.execute_reply":"2024-08-13T11:57:40.143356Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmusamuhammadtukur127\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240813_114021-wb5lrdb7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/musamuhammadtukur127/huggingface/runs/wb5lrdb7' target=\"_blank\">dyu_fr_model</a></strong> to <a href='https://wandb.ai/musamuhammadtukur127/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/musamuhammadtukur127/huggingface' target=\"_blank\">https://wandb.ai/musamuhammadtukur127/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/musamuhammadtukur127/huggingface/runs/wb5lrdb7' target=\"_blank\">https://wandb.ai/musamuhammadtukur127/huggingface/runs/wb5lrdb7</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1209' max='1209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1209/1209 17:01, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>202</td>\n      <td>4.462300</td>\n      <td>4.234174</td>\n      <td>0.408300</td>\n      <td>16.197800</td>\n    </tr>\n    <tr>\n      <td>404</td>\n      <td>4.341000</td>\n      <td>4.122054</td>\n      <td>0.636200</td>\n      <td>16.185400</td>\n    </tr>\n    <tr>\n      <td>606</td>\n      <td>4.222800</td>\n      <td>4.067868</td>\n      <td>0.650100</td>\n      <td>16.384400</td>\n    </tr>\n    <tr>\n      <td>808</td>\n      <td>4.211300</td>\n      <td>4.035388</td>\n      <td>0.783200</td>\n      <td>16.373800</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>4.145700</td>\n      <td>4.026258</td>\n      <td>0.629500</td>\n      <td>16.323000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1209, training_loss=4.292055494436259, metrics={'train_runtime': 1040.6486, 'train_samples_per_second': 18.6, 'train_steps_per_second': 1.162, 'total_flos': 996183746703360.0, 'train_loss': 4.292055494436259, 'epoch': 2.996282527881041})"},"metadata":{}}]},{"cell_type":"code","source":"LOCAL_SAVE_DIR = \"dyu_to_fr_model\"","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:07:22.659837Z","iopub.execute_input":"2024-08-13T12:07:22.660834Z","iopub.status.idle":"2024-08-13T12:07:22.666303Z","shell.execute_reply.started":"2024-08-13T12:07:22.660801Z","shell.execute_reply":"2024-08-13T12:07:22.665071Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"trainer.tokenizer.save_pretrained(LOCAL_SAVE_DIR)\ntrainer.model.save_pretrained(LOCAL_SAVE_DIR)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:07:23.569689Z","iopub.execute_input":"2024-08-13T12:07:23.570066Z","iopub.status.idle":"2024-08-13T12:07:24.955080Z","shell.execute_reply.started":"2024-08-13T12:07:23.570035Z","shell.execute_reply":"2024-08-13T12:07:24.954008Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"prefix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = train['dyu'].iloc[10]\ntext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, AutoTokenizer\nimport torch\n\nmodel = T5ForConditionalGeneration.from_pretrained(LOCAL_SAVE_DIR)\ntokenizer = AutoTokenizer.from_pretrained(LOCAL_SAVE_DIR)\n\ninputs = tokenizer(prefix + text, max_length=300, truncation=True, return_tensors=\"pt\")\noutputs = model.generate(inputs.input_ids, max_new_tokens=40, do_sample=True, top_k=20, top_p=0.7)\noutput_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(output_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['fr'].iloc[10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}