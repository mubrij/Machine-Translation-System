{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9073798,"sourceType":"datasetVersion","datasetId":5473494},{"sourceId":9105282,"sourceType":"datasetVersion","datasetId":5495381}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -q trl evaluate sacrebleu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM,AutoTokenizer\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback\nimport yaml\nfrom datasets import load_from_disk\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\nimport wandb\nimport numpy as np\nfrom datasets import Dataset\nfrom datasets import concatenate_datasets\nimport pandas as pd\nimport seaborn as sns\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import DataCollatorForSeq2Seq\nimport evaluate\n\ndef load_model_and_tokenizer(model_name,tokenizer_name,device_map:str='auto'):\n\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n    \n    model.config.use_cache = False\n\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,trust_remote_code=True)\n\n    tokenizer.padding_side='right'\n\n    return model, tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'google/mt5-small'\nmodel, tokenizer = load_model_and_tokenizer(model_name,model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/melio-dataset/final_train_df.csv\")\ntest = pd.read_csv(\"/kaggle/input/melio-dataset/final_val_df.csv\")\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(tokenizer.encode('Ã…'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"\ntranslate from Dyula to French: {dyu}\n\"\"\"\n\ntrain[\"prompt\"] = train.apply(lambda row: template.format(dyu=row['dyu'],\n                                                             fr=row['fr']),\n                                 axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Markdown\nMarkdown(train[\"prompt\"].iloc[3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_ds_raw = Dataset.from_pandas(train, split=\"train\")\ntrain_ds_raw","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_source_training = train_ds_raw.map(\n    lambda x: tokenizer(x[\"prompt\"], truncation=True), \n    batched=True, remove_columns=['fr', 'dyu', 'prompt'])\n\nsource_lengths_training = [len(x) for x in tokenized_source_training[\"input_ids\"]]\n\nprint(f\"Max source length: {max(source_lengths_training)}\")\nprint(f\"95% source length: {int(np.percentile(source_lengths_training, 95))}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_target_training = train_ds_raw.map(\n    lambda x: tokenizer(x[\"fr\"], truncation=True), \n    batched=True, remove_columns=['fr', 'dyu', 'prompt'])\ntarget_lengths_training = [len(x) for x in tokenized_target_training[\"input_ids\"]]\n\nprint(f\"Max target length: {max(target_lengths_training)}\")\nprint(f\"95% target length: {int(np.percentile(target_lengths_training, 95))}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_source_length = max(source_lengths_training)\nmax_source_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_target_length = max(target_lengths_training)\nmax_target_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(sample, padding=\"max_length\"):\n\n    model_inputs = tokenizer(sample[\"prompt\"], max_length=max_source_length, padding=padding, truncation=True)\n    labels = tokenizer(text_target=sample[\"fr\"], max_length=max_target_length, padding=padding, truncation=True)\n    \n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_train_ds = train_ds_raw.map(\n    preprocess_function, batched=True, \n    remove_columns=['fr', 'dyu', 'prompt'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_train_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_dict = tokenized_train_ds.train_test_split(test_size=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainset = ds_dict[\"train\"]\ntrainset   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testset = ds_dict[\"test\"]\ntestset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport numpy as np\nfrom nltk.tokenize import sent_tokenize\nnltk.download(\"punkt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Metric\nmetric = evaluate.load(\"sacrebleu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function to postprocess text\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n\n    # Replace -100 in the labels as we can't decode them.\n    # for some reason, also get a lot of -100 in preds\n    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)    \n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\n# we want to ignore tokenizer pad token in the loss\nlabel_pad_token_id = -100\n\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n    pad_to_multiple_of=8\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfFolder\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 8\npath = \"dyu_to_fr_model\"\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=path,\n    learning_rate=5e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    generation_max_length=273,\n    weight_decay=0.01,\n    num_train_epochs=10,\n    predict_with_generate=True,\n    fp16=False,\n    #bf16=True,\n    # logging & evaluation strategies\n   # logging_dir=f\"{repository_id}/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=500,\n    evaluation_strategy=\"steps\",\n    save_strategy=\"steps\",\n    save_total_limit=2,\n    load_best_model_at_end=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=trainset,\n    eval_dataset=testset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOCAL_SAVE_DIR = \"dyu_to_fr_model\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.tokenizer.save_pretrained(LOCAL_SAVE_DIR)\ntrainer.model.save_pretrained(LOCAL_SAVE_DIR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = train['dyu'].iloc[10]\ntext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, AutoTokenizer\nimport torch\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(LOCAL_SAVE_DIR)\ntokenizer = AutoTokenizer.from_pretrained(LOCAL_SAVE_DIR)\n\ninputs = tokenizer(prefix + text, max_length=tokenizer.model_max_length, return_tensors=\"pt\")\noutputs = model.generate(inputs.input_ids, max_new_tokens=40, do_sample=True, top_k=20, top_p=0.7)\noutput_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(output_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['fr'].iloc[10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}